#!/usr/bin/env python

import json
import os
import sys
import glob
from cloudpickle import dumps, loads
import numpy as np
import dask.array as da
import dask.dataframe as ddf
import argparse
from random import randint

from pathlib import Path

from dask_ml.wrappers import ParallelPostFit

from dask_ml.ensemble import BlockwiseVotingClassifier

from dask_ml.compose import ColumnTransformer

from sklearnex import patch_sklearn
patch_sklearn()

from sklearn.ensemble import HistGradientBoostingClassifier

from dask_ml.preprocessing import MinMaxScaler

from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from sklearn.linear_model import LogisticRegression


labels = ['adware', 'flooder', 'ransomware', 'dropper', 'spyware', 'packed', 'crypto_miner', 'file_infector', 'installer', 'worm', 'downloader']
features = [f"feature_{x}" for x in range(2381)]
prefix="/opt/ml"

def log_container_info():
    print('------------------- environment variables -------------------')
    print(os.environ)
    print('------------------- environment variables -------------------')
    print('------------------- arguments -------------------')
    print(sys.argv)
    print('------------------- arguments -------------------')
    print('------------------- filesystem -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        print(filename)
    print('------------------- filesystem -------------------')
    print('------------------- config -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        if filename.endswith(".json"):
            print(f'------------------- {filename} -------------------')
            with open(filename, "r") as f:
                print(f.read())
            print(f'------------------- {filename} -------------------')
    print('------------------- config -------------------')


def train(args, hyperparameters):
    print('Starting the training.')

    print("load k best")
    kbest = set()
    for label in labels:
        with open(f"{prefix}/input/data/support/kbest/scores-{label}.pkl", 'rb') as f:
            scores = loads(f.read())
            columns = [x[0] for x in list(sorted(scores, key=lambda i: i[1], reverse=True))[:args.k]]
            kbest.update(columns)

    print(f"k best length = {len(kbest)}")

    print("load train and test dataset")
    X_train = ddf.read_parquet(f"{prefix}/input/data/train/x/", compression="snappy", columns=list(kbest))
    X_test  = ddf.read_parquet(f"{prefix}/input/data/test/x/" , compression="snappy", columns=list(kbest))
    y_train = ddf.read_parquet(f"{prefix}/input/data/train/y/", compression="snappy", columns=labels)
    y_test  = ddf.read_parquet(f"{prefix}/input/data/test/y/" , compression="snappy", columns=labels)

    print("sample train")
    if args.sample < 1.0:
        X_train = X_train.sample(frac=args.sample, replace=True, random_state=args.random_state)
        y_train = y_train.sample(frac=args.sample, replace=True, random_state=args.random_state)


    print("X train to array")
    X_train = X_train.to_dask_array(lengths=True)

    print("X test to array")
    X_test = X_test.to_dask_array(lengths=True)

    print("training hgbc models")    
    training_true = dict()
    test_true = dict()
    training_pred = dict()
    test_pred = dict()
    for label in labels:
        clf = train_hgbc(args, hyperparameters, label, X_train, y_train)
        training_true[label] = y_train[label].to_dask_array(lengths=True).compute()
        test_true[label]     = y_test[label].to_dask_array(lengths=True).compute()
        training_pred[label] = clf.predict(X_train).compute()
        test_pred[label]     = clf.predict(X_test).compute()

    for label in labels:
        y_training_true = training_true[label]
        y_test_true     = test_true[label]

        y_training_pred = training_pred[label]
        y_test_pred     = test_pred[label]

        print(f"----------------------------------{label}----------------------------------")
        print(f"{label}_training_accuracy_score={accuracy_score(y_training_true, y_training_pred):.5f};")
        print(f"{label}_training_balanced_accuracy_score={balanced_accuracy_score(y_training_true, y_training_pred, adjusted=True):.5f};")
        print(f"{label}_training_precision_score={precision_score(y_training_true, y_training_pred):.5f};")
        print(f"{label}_training_recall_score={recall_score(y_training_true, y_training_pred):.5f};")
        print(f"{label}_training_f1_score={f1_score(y_training_true, y_training_pred):.5f};")

        print(f"{label}_test_accuracy_score={accuracy_score(y_test_true, y_test_pred):.5f};")
        print(f"{label}_test_balanced_accuracy_score={balanced_accuracy_score(y_test_true, y_test_pred, adjusted=True):.5f};")
        print(f"{label}_test_precision_score={precision_score(y_test_true, y_test_pred):.5f};")
        print(f"{label}_test_recall_score={recall_score(y_test_true, y_test_pred):.5f};")
        print(f"{label}_test_f1_score={f1_score(y_test_true, y_test_pred):.5f};")


    y_training_true = da.concatenate([training_true[label] for label in labels])
    y_test_true     = da.concatenate([test_true[label] for label in labels])

    y_training_pred = da.concatenate([training_pred[label] for label in labels])
    y_test_pred     = da.concatenate([test_pred[label] for label in labels])

    print(f"----------------------------------General----------------------------------")
    print(f"training_accuracy_score={accuracy_score(y_training_true, y_training_pred):.5f};")
    print(f"training_balanced_accuracy_score={balanced_accuracy_score(y_training_true, y_training_pred, adjusted=True):.5f};")
    print(f"training_precision_score={precision_score(y_training_true, y_training_pred):.5f};")
    print(f"training_recall_score={recall_score(y_training_true, y_training_pred):.5f};")
    print(f"training_f1_score={f1_score(y_training_true, y_training_pred):.5f};")

    print(f"test_accuracy_score={accuracy_score(y_test_true, y_test_pred):.5f};")
    print(f"test_balanced_accuracy_score={balanced_accuracy_score(y_test_true, y_test_pred, adjusted=True):.5f};")
    print(f"test_precision_score={precision_score(y_test_true, y_test_pred):.5f};")
    print(f"test_recall_score={recall_score(y_test_true, y_test_pred):.5f};")
    print(f"test_f1_score={f1_score(y_test_true, y_test_pred):.5f};")
    
    print('Finish training')

def train_hgbc(args, hyperparameters, label, X_train, y_train):
    print(f"training model for {label}")
    modelname = f"{prefix}/output/data/model-{label}-{args.sample}-{args.k}.pkl"
    if Path(modelname).is_file():
        print(f"loading {modelname} from disk")
        with open(modelname, "rb") as f:
            return loads(f.read())

    y_training_true = y_train[label].to_dask_array(lengths=True)

    clf = ParallelPostFit(estimator=HistGradientBoostingClassifier(**hyperparameters))
    clf.fit(X_train, y_training_true)
    
    print("save model")
    with open(modelname, "wb") as f:
        f.write(dumps(clf))

    return clf


def cast(hyperparameters, name, type):
    if name in hyperparameters:
        hyperparameters[name] = type(hyperparameters[name])

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("-k", type=int, default=100)
    parser.add_argument("-sample", type=float, default=0.1)
    parser.add_argument("-random-state", type=int, default=randint(0, 2**32))
    with open('/opt/ml/input/config/hyperparameters.json', 'r') as f:
        hyperparameters = json.load(f)
        cast(hyperparameters, "random_state", int)
        cast(hyperparameters, "verbose", int)
        cast(hyperparameters, "n_jobs", int)

    args, unknown = parser.parse_known_args()
    log_container_info()
    train(args, hyperparameters)

    sys.exit(0)
