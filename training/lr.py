#!/usr/bin/env python

import json
import os
import sys
import glob
from cloudpickle import dumps, loads
import dask.dataframe as ddf
import argparse
from random import randint

from sklearnex import patch_sklearn
patch_sklearn()

from sklearn.linear_model import LogisticRegression


labels = ['adware', 'flooder', 'ransomware', 'dropper', 'spyware', 'packed', 'crypto_miner', 'file_infector', 'installer', 'worm', 'downloader']
prefix="/opt/ml"

def log_container_info():
    print('------------------- environment variables -------------------')
    print(os.environ)
    print('------------------- environment variables -------------------')
    print('------------------- arguments -------------------')
    print(sys.argv)
    print('------------------- arguments -------------------')
    print('------------------- filesystem -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        print(filename)
    print('------------------- filesystem -------------------')
    print('------------------- config -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        if filename.endswith(".json"):
            print(f'------------------- {filename} -------------------')
            with open(filename, "r") as f:
                print(f.read())
            print(f'------------------- {filename} -------------------')
    print('------------------- config -------------------')

def train(args, hyperparameters):
    print('Starting the training.')

    print(f"load score for {args.label}")
    with open(f"{prefix}/input/data/training/k-{args.label}/scores-{args.label}.pkl", 'rb') as f:
        scores = loads(f.read())
        
        columns = [x[0] for x in list(sorted(scores, key=lambda i: i[1], reverse=True))[:args.k]]
        print(columns)

    print("load X")
    X = ddf.read_parquet(f"{prefix}/input/data/training/{args.data_folder}/{args.x_name}", compression="snappy", columns=columns)

    print("load y")
    y = ddf.read_parquet(f"{prefix}/input/data/training/{args.data_folder}/{args.y_name}", compression="snappy", columns=[args.label])

    clf = LogisticRegression(**hyperparameters).fit(X, y.values.compute().ravel())

    print(f"score training={clf.score(X, y)}")

    print("save model")
    with open(f"{prefix}/output/data/model-{args.label}.pkl", "wb") as f:
        f.write(dumps(clf))

    print('Finish the processing.')

def cast(hyperparameters, name, type):
    if name in hyperparameters:
        hyperparameters[name] = type(hyperparameters[name])

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("-data-folder", type=str)
    parser.add_argument("-x-name", type=str, default='x-train')
    parser.add_argument("-y-name", type=str, default='y-train')
    parser.add_argument("-label", type=str, choices=labels)
    parser.add_argument("-k", type=int, default=100)
    with open('/opt/ml/input/config/hyperparameters.json', 'r') as f:
        hyperparameters = json.load(f)
        cast(hyperparameters, "random_state", int)
        cast(hyperparameters, "dual", bool)
        cast(hyperparameters, "tol", float)
        cast(hyperparameters, "C", float)
        cast(hyperparameters, "fit_intercept", bool)
        cast(hyperparameters, "intercept_scaling", float)
        cast(hyperparameters, "max_iter", int)
        cast(hyperparameters, "verbose", int)
        cast(hyperparameters, "warm_start", bool)
        cast(hyperparameters, "n_jobs", int)
        cast(hyperparameters, "l1_ratio", float)

    args = parser.parse_args()
    log_container_info()
    train(args, hyperparameters)

    sys.exit(0)
    