#!/usr/bin/env python

import json
import os
import sys
import glob
from cloudpickle import dumps, loads
import dask.dataframe as ddf
import argparse
from random import randint

from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from sklearnex import patch_sklearn
patch_sklearn()

from sklearn.linear_model import LogisticRegression


labels = ['adware', 'flooder', 'ransomware', 'dropper', 'spyware', 'packed', 'crypto_miner', 'file_infector', 'installer', 'worm', 'downloader']
prefix="/opt/ml"

def log_container_info():
    print('------------------- environment variables -------------------')
    print(os.environ)
    print('------------------- environment variables -------------------')
    print('------------------- arguments -------------------')
    print(sys.argv)
    print('------------------- arguments -------------------')
    print('------------------- filesystem -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        print(filename)
    print('------------------- filesystem -------------------')
    print('------------------- config -------------------')
    for filename in glob.iglob(prefix + '**/**', recursive=True):
        if filename.endswith(".json"):
            print(f'------------------- {filename} -------------------')
            with open(filename, "r") as f:
                print(f.read())
            print(f'------------------- {filename} -------------------')
    print('------------------- config -------------------')

def train(args, hyperparameters):
    print('Starting the training.')

    print(f"load score for {args.label}")
    with open(f"{prefix}/input/data/training/k-{args.label}/scores-{args.label}.pkl", 'rb') as f:
        scores = loads(f.read())

        columns = [x[0] for x in list(sorted(scores, key=lambda i: i[1], reverse=True))[:args.k]]
        print(columns)

    print("load X")
    X = ddf.read_parquet(f"{prefix}/input/data/training/{args.data_folder}/{args.x_train}", compression="snappy", columns=columns)

    print("load y")
    y = ddf.read_parquet(f"{prefix}/input/data/training/{args.data_folder}/{args.y_train}", compression="snappy", columns=[args.label])
    training_y_true = y.values.compute().ravel()

    clf = LogisticRegression(**hyperparameters).fit(X, training_y_true)

    training_y_pred = clf.predict(X)

    print(f"training_accuracy_score={accuracy_score(training_y_true, training_y_pred):.3f};")
    print(f"training_balanced_accuracy_score={balanced_accuracy_score(training_y_true, training_y_pred, adjusted=True):.3f};")
    print(f"training_precision_score={precision_score(training_y_true, training_y_pred):.3f};")
    print(f"training_recall_score={recall_score(training_y_true, training_y_pred):.3f};")
    print(f"training_f1_score={f1_score(training_y_true, training_y_pred):.3f};")
    print(f"training_roc_auc_score={roc_auc_score(training_y_true, training_y_pred):.3f};")

    print("load X test")
    X_test = ddf.read_parquet(f"{prefix}/input/data/training/{args.data_folder}/{args.x_test}", compression="snappy", columns=columns)

    print("load y test")
    y_test = ddf.read_parquet(f"{prefix}/input/data/training/{args.data_folder}/{args.y_test}", compression="snappy", columns=[args.label])
    test_y_true = y_test.values.compute().ravel()

    test_y_pred = clf.predict(X_test)

    print(f"test_accuracy_score={accuracy_score(test_y_true, test_y_pred):.3f};")
    print(f"test_balanced_accuracy_score={balanced_accuracy_score(test_y_true, test_y_pred, adjusted=True):.3f};")
    print(f"test_precision_score={precision_score(test_y_true, test_y_pred):.3f};")
    print(f"test_recall_score={recall_score(test_y_true, test_y_pred):.3f};")
    print(f"test_f1_score={f1_score(test_y_true, test_y_pred):.3f};")
    print(f"test_roc_auc_score={roc_auc_score(test_y_true, test_y_pred):.3f};")

    print("save model")
    with open(f"{prefix}/output/data/model-{args.label}.pkl", "wb") as f:
        f.write(dumps(clf))

    print('Finish the processing.')

def cast(hyperparameters, name, type):
    if name in hyperparameters:
        hyperparameters[name] = type(hyperparameters[name])

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("-data-folder", type=str)
    parser.add_argument("-x-train", type=str, default='x-train')
    parser.add_argument("-y-train", type=str, default='y-train')
    parser.add_argument("-x-test", type=str, default='x-test')
    parser.add_argument("-y-test", type=str, default='y-test')
    parser.add_argument("-label", type=str, choices=labels)
    parser.add_argument("-k", type=int, default=100)
    with open('/opt/ml/input/config/hyperparameters.json', 'r') as f:
        hyperparameters = json.load(f)
        cast(hyperparameters, "random_state", int)
        cast(hyperparameters, "dual", bool)
        cast(hyperparameters, "tol", float)
        cast(hyperparameters, "C", float)
        cast(hyperparameters, "fit_intercept", bool)
        cast(hyperparameters, "intercept_scaling", float)
        cast(hyperparameters, "max_iter", int)
        cast(hyperparameters, "verbose", int)
        cast(hyperparameters, "warm_start", bool)
        cast(hyperparameters, "n_jobs", int)
        cast(hyperparameters, "l1_ratio", float)

    args = parser.parse_args()
    log_container_info()
    train(args, hyperparameters)

    sys.exit(0)
